diff --git a/.gitignore b/.gitignore
index 7314b75..29ad152 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,3 @@
 *.pyc
-agent_diy
+agent_diy/
 .vscode/
\ No newline at end of file
diff --git a/agent_ppo/agent.py b/agent_ppo/agent.py
index e3c3291..5c98eae 100644
--- a/agent_ppo/agent.py
+++ b/agent_ppo/agent.py
@@ -1,360 +1,635 @@
 #!/usr/bin/env python3
 # -*- coding: UTF-8 -*-
-###########################################################################
-# Copyright © 1998 - 2025 Tencent. All Rights Reserved.
-###########################################################################
-"""
-Author: Tencent AI Arena Authors
-"""
-
 
 import torch
+import numpy as np
 
 torch.set_num_threads(1)
 torch.set_num_interop_threads(1)
 
-import os
-from agent_ppo.model.model import Model
-from agent_ppo.feature.definition import *
-import numpy as np
 from kaiwu_agent.agent.base_agent import (
-    BaseAgent,
     predict_wrapper,
     exploit_wrapper,
     learn_wrapper,
     save_model_wrapper,
     load_model_wrapper,
+    BaseAgent,
 )
-
-from agent_ppo.conf.conf import Config
+from agent_diy.model.model import Model
 from kaiwu_agent.utils.common_func import attached
-from agent_ppo.feature.reward_process import GameRewardManager
-from torch.optim.lr_scheduler import LambdaLR
-from agent_ppo.algorithm.algorithm import Algorithm
+from agent_diy.feature.definition import *
+from agent_diy.conf.conf import Config, GameConfig
+from agent_diy.algorithm.algorithm import Algorithm
 
 
 @attached
 class Agent(BaseAgent):
     def __init__(self, agent_type="player", device=None, logger=None, monitor=None):
+        super().__init__(agent_type, device, logger, monitor)
         self.cur_model_name = ""
         self.device = device
-        # Create Model and convert the model to achannel-last memory format to achieve better performance.
-        # 创建模型, 将模型转换为通道后内存格式，以获得更好的性能。
+        
+        # 创建模型
         self.model = Model().to(self.device)
         self.model = self.model.to(memory_format=torch.channels_last)
 
-        # config info
-        # 配置信息
-        self.lstm_unit_size = Config.LSTM_UNIT_SIZE
-        self.lstm_hidden = np.zeros([self.lstm_unit_size])
-        self.lstm_cell = np.zeros([self.lstm_unit_size])
-        self.label_size_list = Config.LABEL_SIZE_LIST
-        self.legal_action_size = Config.LEGAL_ACTION_SIZE_LIST
-        self.seri_vec_split_shape = Config.SERI_VEC_SPLIT_SHAPE
-
-        # env info
         # 环境信息
         self.hero_camp = 0
         self.player_id = 0
         self.game_id = None
 
-        # learning info
-        # 学习信息
-        self.train_step = 0
-        self.lr = Config.INIT_LEARNING_RATE_START
-        parameters = self.model.parameters()
-        self.optimizer = torch.optim.Adam(params=parameters, lr=self.lr, betas=(0.9, 0.999), eps=1e-8)
-        self.parameters = [p for param_group in self.optimizer.param_groups for p in param_group["params"]]
-        self.target_lr = Config.TARGET_LR
-        self.target_step = Config.TARGET_STEP
-        self.scheduler = LambdaLR(self.optimizer, lr_lambda=self.lr_lambda)
-
-        # tools
         # 工具
         self.reward_manager = None
         self.logger = logger
         self.monitor = monitor
-
-        # predict local or remote
-        # 本地预测或远程预测
-        self.is_predict_remote = True
-
-        self.algorithm = Algorithm(self.model, self.optimizer, self.scheduler, self.device, self.logger, self.monitor)
-
-        super().__init__(agent_type, device, logger, monitor)
-
-    def lr_lambda(self, step):
-        # Define learning rate decay function
-        # 定义学习率衰减函数
-        if step > self.target_step:
-            return self.target_lr / self.lr
-        else:
-            return 1.0 - ((1.0 - self.target_lr / self.lr) * step / self.target_step)
+        self.algorithm = Algorithm(self.model, self.device, self.logger, self.monitor)
+        
+        # 生存策略状态
+        self.survival_state = "normal"  # normal, need_retreat, retreating, recalling, recovering
+        self.state_counter = 0
+        self.last_hp_ratio = 1.0
+        self.recall_cooldown = 0
+        self.last_recall_frame = 0
+        self.continuous_low_hp_frames = 0
+        self.last_distance_to_spring = float('inf')
+        self.combat_frames = 0
+        
+        # 战斗状态追踪
+        self.in_combat = False
+        self.last_damage_frame = 0
+        self.enemy_distance = float('inf')
+        
+        # 动作历史
+        self.action_history = []
+        self.max_history_length = 10
+        
+        # 新增：技能后平A状态追踪 [修改点1]
+        self.pending_normal_attack = False
 
     def _model_inference(self, list_obs_data):
-        # Using the network for inference
-        # 使用网络进行推理
-        feature = [obs_data.feature for obs_data in list_obs_data]
-        legal_action = [obs_data.legal_action for obs_data in list_obs_data]
-        lstm_cell = [obs_data.lstm_cell for obs_data in list_obs_data]
-        lstm_hidden = [obs_data.lstm_hidden for obs_data in list_obs_data]
-
-        input_list = [np.array(feature), np.array(lstm_cell), np.array(lstm_hidden)]
-        torch_inputs = [torch.from_numpy(nparr).to(torch.float32) for nparr in input_list]
-        for i, data in enumerate(torch_inputs):
-            data = data.reshape(-1)
-            torch_inputs[i] = data.float()
-
-        feature, lstm_cell, lstm_hidden = torch_inputs
-        feature_vec = feature.reshape(-1, self.seri_vec_split_shape[0][0])
-        lstm_hidden_state = lstm_hidden.reshape(-1, self.lstm_unit_size)
-        lstm_cell_state = lstm_cell.reshape(-1, self.lstm_unit_size)
-
-        format_inputs = [feature_vec, lstm_hidden_state, lstm_cell_state]
-
-        self.model.set_eval_mode()
-        with torch.no_grad():
-            output_list = self.model(format_inputs, inference=True)
-
-        np_output = []
-        for output in output_list:
-            np_output.append(output.numpy())
-
-        logits, value, _lstm_cell, _lstm_hidden = np_output[:4]
-
-        _lstm_cell = _lstm_cell.squeeze(axis=0)
-        _lstm_hidden = _lstm_hidden.squeeze(axis=0)
-
-        list_act_data = list()
-        for i in range(len(legal_action)):
-            prob, action, d_action = self._sample_masked_action(logits[i], legal_action[i])
-            list_act_data.append(
-                ActData(
-                    action=action,
-                    d_action=d_action,
-                    prob=prob,
-                    value=value,
-                    lstm_cell=_lstm_cell[i],
-                    lstm_hidden=_lstm_hidden[i],
-                )
-            )
+        """模型推理"""
+        # 增强观察数据
+        for obs_data in list_obs_data:
+            obs_data = self.enhance_observation(obs_data)
+        
+        # 基础模型推理
+        list_act_data = self.model.forward(list_obs_data)
+        
+        # 应用生存策略增强
+        for i, act_data in enumerate(list_act_data):
+            act_data = self.apply_survival_strategy(act_data, list_obs_data[i])
+            list_act_data[i] = act_data
+        
         return list_act_data
 
+    def enhance_observation(self, obs_data):
+        """增强观察数据，添加生存相关特征"""
+        if not hasattr(obs_data, 'survival_features'):
+            survival_features = []
+            
+            # 添加生存状态编码
+            state_encoding = {
+                "normal": [1, 0, 0, 0, 0],
+                "need_retreat": [0, 1, 0, 0, 0],
+                "retreating": [0, 0, 1, 0, 0],
+                "recalling": [0, 0, 0, 1, 0],
+                "recovering": [0, 0, 0, 0, 1]
+            }
+            survival_features.extend(state_encoding.get(self.survival_state, [1, 0, 0, 0, 0]))
+            
+            # 添加血量阈值标记
+            survival_features.append(1.0 if self.last_hp_ratio < 0.35 else 0.0)
+            survival_features.append(1.0 if self.last_hp_ratio < 0.25 else 0.0)
+            
+            # 添加战斗状态
+            survival_features.append(1.0 if self.in_combat else 0.0)
+            survival_features.append(min(1.0, self.combat_frames / 100.0))
+            
+            # 添加回城冷却
+            survival_features.append(0.0 if self.recall_cooldown > 0 else 1.0)
+            
+            obs_data.survival_features = np.array(survival_features, dtype=np.float32)
+        
+        return obs_data
+
+    def apply_survival_strategy(self, act_data, obs_data):
+        """应用生存策略到动作"""
+        # 获取当前血量状态
+        hp_ratio = getattr(obs_data, 'hp_ratio', 1.0)
+        spring_distance = getattr(obs_data, 'spring_distance', float('inf'))
+        
+        # 强制回城逻辑
+        if self.should_force_recall(hp_ratio, spring_distance):
+            act_data.action[0] = Config.ACTION_MAPPING["recall"]
+            if self.logger:
+                self.logger.info(f"Force recall: HP={hp_ratio:.2f}, State={self.survival_state}")
+        
+        # 撤退逻辑
+        elif self.should_retreat(hp_ratio):
+            act_data = self.apply_retreat_action(act_data, obs_data)
+        
+        # 谨慎进攻逻辑
+        elif hp_ratio < 0.5 and self.in_combat:
+            act_data = self.apply_defensive_action(act_data, obs_data)
+        
+        return act_data
+
+    def should_force_recall(self, hp_ratio, spring_distance):
+        """判断是否强制回城"""
+        # 条件1：极低血量且脱战
+        if hp_ratio < 0.2 and not self.in_combat and self.recall_cooldown == 0:
+            return True
+        
+        # 条件2：低血量持续时间过长
+        if hp_ratio < 0.3:
+            self.continuous_low_hp_frames += 1
+            if self.continuous_low_hp_frames > 150 and not self.in_combat:  # 5秒
+                return True
+        else:
+            self.continuous_low_hp_frames = 0
+        
+        # 条件3：在撤退状态但血量继续下降
+        if self.survival_state == "retreating" and hp_ratio < self.last_hp_ratio - 0.1:
+            if not self.in_combat and self.recall_cooldown == 0:
+                return True
+        
+        return False
+
+    def should_retreat(self, hp_ratio):
+        """判断是否需要撤退"""
+        # 血量低于30%开始考虑撤退 [修改点4：从0.35改为0.30]
+        retreat_threshold = 0.30
+        
+        if hp_ratio < retreat_threshold:
+            if self.survival_state == "normal":
+                self.survival_state = "need_retreat"
+                self.state_counter = 0
+            return True
+        
+        # 血量恢复到安全线
+        if hp_ratio > GameConfig.RECALL_CONFIG["safe_hp_threshold"]:
+            # [修改关键点] 移除了 "recovering"，防止配置表设置70%安全线时把正在恢复(需要90%)的Agent踢出泉水
+            if self.survival_state in ["retreating"]:
+                self.survival_state = "normal"
+        
+        return self.survival_state in ["need_retreat", "retreating"]
+
+    def apply_retreat_action(self, act_data, obs_data):
+        """应用撤退动作"""
+        # 获取泉水方向
+        spring_direction = getattr(obs_data, 'spring_direction', 4)  # 默认后退
+        
+        # 使用位移技能逃生
+        if self.survival_state == "need_retreat":
+            # 优先使用位移技能
+            if getattr(obs_data, 'skill_2_ready', False):  # 鲁班的2技能有击退
+                act_data.action[0] = Config.ACTION_MAPPING["skill_2"]
+            else:
+                # 向泉水方向移动
+                act_data.action[0] = spring_direction
+            
+            self.survival_state = "retreating"
+        
+        return act_data
+
+    def apply_defensive_action(self, act_data, obs_data):
+        """应用防守型动作"""
+        # 保持距离，避免近战
+        enemy_direction = getattr(obs_data, 'enemy_direction', 0)
+        
+        # 反方向移动
+        opposite_direction = (enemy_direction + 4) % 8
+        act_data.action[0] = opposite_direction
+        
+        # 使用控制技能
+        if self.enemy_distance < 5000:
+            if getattr(obs_data, 'skill_2_ready', False):
+                act_data.action[0] = Config.ACTION_MAPPING["skill_2"]
+        
+        return act_data
+
     @predict_wrapper
     def predict(self, observation):
-        # The remote prediction will not call agent.reset in the workflow. Users can use the game_id to determine whether a new environment
-        # 远程预测不会在workflow中重置agent，用户可以通过game_id判断是否是新的对局，并根据新对局对agent进行重置
+        """预测动作"""
         game_id = observation["game_id"]
         if self.game_id != game_id:
-            player_id = observation["player_id"]
-            camp = observation["player_camp"]
-            self.reset(camp, player_id)
+            self.reset()
             self.game_id = game_id
 
-        # exploit is automatically called when submitting an evaluation task.
-        # The parameter is the observation returned by env, and it returns the action used by env.step.
-        # exploit在提交评估任务时自动调用，参数为env返回的state_dict, 返回env.step使用的action
         obs_data = self.observation_process(observation)
-        # Call _model_inference for model inference, executing local model inference
-        # 模型推理调用_model_inference, 执行本地模型推理
         act_data = self._model_inference([obs_data])[0]
-        self.update_status(obs_data, act_data)
+        self.update_status(obs_data, act_data, observation)
         action = self.action_process(observation, act_data, False)
+        
         return [ActData(action=action)]
 
     @exploit_wrapper
     def exploit(self, observation):
-        # Evaluation task will not call agent.reset in the workflow. Users can use the game_id to determine whether a new environment
-        # 评估任务不会在workflow中重置agent，用户可以通过game_id判断是否是新的对局，并根据新对局对agent进行重置
+        """评估时的动作"""
         game_id = observation["game_id"]
         if self.game_id != game_id:
-            player_id = observation["player_id"]
-            camp = observation["player_camp"]
-            self.reset(camp, player_id)
+            self.reset()
             self.game_id = game_id
 
-        # exploit is automatically called when submitting an evaluation task.
-        # The parameter is the observation returned by env, and it returns the action used by env.step.
-        # exploit在提交评估任务时自动调用，参数为env返回的state_dict, 返回env.step使用的action
         obs_data = self.observation_process(observation)
-        # Call _model_inference for model inference, executing local model inference
-        # 模型推理调用_model_inference, 执行本地模型推理
         act_data = self._model_inference([obs_data])[0]
-        self.update_status(obs_data, act_data)
+        self.update_status(obs_data, act_data, observation)
         d_action = self.action_process(observation, act_data, False)
+        
         return [ActData(d_action=d_action)]
 
     def train_predict(self, observation):
-        # Call agent.predict for distributed model inference
-        # 调用agent.predict，执行分布式模型推理
+        """训练时的预测"""
         if self.is_predict_remote:
             act_data, model_version = self.predict(observation)
             return act_data[0].action
 
         obs_data = self.observation_process(observation)
         act_data = self._model_inference([obs_data])[0]
-        self.update_status(obs_data, act_data)
+        self.update_status(obs_data, act_data, observation)
         return self.action_process(observation, act_data, True)
 
     def eval_predict(self, observation):
-        # Call agent.predict for distributed model inference
-        # 调用agent.predict，执行分布式模型推理
+        """评估时的预测"""
         if self.is_predict_remote:
             act_data, model_version = self.exploit(observation)
             return act_data[0].d_action
 
         obs_data = self.observation_process(observation)
         act_data = self._model_inference([obs_data])[0]
-        self.update_status(obs_data, act_data)
+        self.update_status(obs_data, act_data, observation)
         return self.action_process(observation, act_data, False)
 
-    def action_process(self, observation, act_data, is_stochastic):
-        if is_stochastic:
-            # Use stochastic sampling action
-            # 采用随机采样动作 action
-            return act_data.action
-        else:
-            # Use the action with the highest probability
-            # 采用最大概率动作 d_action
-            return act_data.d_action
-
-    def observation_process(self, observation):
-        feature_vec, legal_action = (
-            observation["observation"],
-            observation["legal_action"],
-        )
-        return ObsData(
-            feature=feature_vec, legal_action=legal_action, lstm_cell=self.lstm_cell, lstm_hidden=self.lstm_hidden
-        )
-
     @learn_wrapper
     def learn(self, list_sample_data):
         return self.algorithm.learn(list_sample_data)
 
+    def action_process(self, observation, act_data, is_stochastic):
+        """动作处理 - 核心生存策略实现"""
+        action = act_data.action
+        
+        # --------------------------------------------------------
+        # [修改点1] 强制技能衔接普攻逻辑 (最高优先级)
+        # 如果上一步使用了技能，这一步强制普攻
+        # --------------------------------------------------------
+        if self.pending_normal_attack:
+            action[0] = Config.ACTION_MAPPING["attack"]  # 9
+            self.pending_normal_attack = False
+            # 记录并返回，跳过后续决策
+            self.action_history.append(action[0])
+            if len(self.action_history) > self.max_history_length:
+                self.action_history.pop(0)
+            return action
+        # --------------------------------------------------------
+
+        # 提取关键信息
+        hero_state = self.extract_hero_state(observation)
+        hp_ratio = hero_state.get("hp_ratio", 1.0)
+        spring_distance = hero_state.get("spring_distance", float('inf'))
+        enemy_distance = hero_state.get("enemy_distance", float('inf'))
+        
+        # 获取血包信息 [修改点2]
+        pack_available = hero_state.get("pack_available", False)
+        pack_distance = hero_state.get("pack_distance", float('inf'))
+        pack_direction = hero_state.get("pack_direction", 4)
+        
+        # 更新战斗状态
+        self.update_combat_state(hero_state)
+        
+        # 更新回城冷却
+        if self.recall_cooldown > 0:
+            self.recall_cooldown -= 1
+        
+        # 决策优先级
+        # 1. 极低血量强制回城
+        if hp_ratio < 0.2 and not self.in_combat and self.recall_cooldown == 0:
+            action[0] = Config.ACTION_MAPPING["recall"]
+            self.recall_cooldown = 300
+            self.survival_state = "recalling"
+            if self.logger:
+                self.logger.info(f"Emergency recall at {hp_ratio:.1%} HP")
+
+        # --------------------------------------------------------
+        # [修改点2] 低血量(<40%)优先吃塔下血包
+        # 如果血量低且血包可用，强制向血包移动
+        # --------------------------------------------------------
+        elif (hp_ratio < 0.40 and 
+              pack_available and 
+              pack_distance < 6000 and 
+              not self.in_combat):
+            action[0] = pack_direction
+            # 不改变 survival_state 为 retreating，只是一种临时的资源获取行为
+        # --------------------------------------------------------
+        
+        # 2. 低血量撤退 [修改点4：阈值从0.35改为0.30]
+        elif hp_ratio < 0.30 and self.survival_state == "normal":
+            self.survival_state = "need_retreat"
+            # 使用位移技能撤退
+            if hero_state.get("skill_2_ready", False):
+                action[0] = Config.ACTION_MAPPING["skill_2"]
+            else:
+                # 向泉水方向移动
+                action = self.move_towards_spring(action, hero_state)
+        
+        # 3. 恢复中
+        elif self.survival_state == "recovering":
+            # [修改关键点] 必须恢复到 90% 血量才能离开
+            if hp_ratio > 0.9:
+                self.survival_state = "normal"
+            elif spring_distance > 2000:
+                # 离泉水太远，强制往泉水跑
+                action = self.move_towards_spring(action, hero_state)
+            else:
+                # [新增关键逻辑] 在泉水范围内 (<=2000) 且血没满 90%，强制待机
+                # 修复"站1秒就走"的问题
+                action[0] = Config.ACTION_MAPPING["no_op"]
+        
+        # 4. 正常战斗，但根据血量调整激进程度
+        elif self.survival_state == "normal":
+            # ----------------------------------------------------
+            # [修改点3] 有小兵和敌方英雄时，优先攻击敌方英雄
+            # 如果敌方英雄在普攻范围内 (4500)，强制攻击
+            # ----------------------------------------------------
+            if enemy_distance < 4500:
+                action[0] = Config.ACTION_MAPPING["attack"] # 9
+                # 注意：此处假设游戏内核或 action target 能够处理具体的英雄锁定
+                # 如果没有显式的 target 设定，强制攻击指令通常会攻击最近的单位
+                # 若需要强制锁定，需设置 action 的 target 字段（视具体接口定义而定）
+                # 这里我们强制输出攻击动作
+            # ----------------------------------------------------
+            elif hp_ratio < 0.5 and enemy_distance < 5000:
+                # 保守策略
+                action = self.apply_conservative_strategy(action, hero_state)
+        
+        # --------------------------------------------------------
+        # [修改点1] 检测技能施放，设置 pending_normal_attack
+        # --------------------------------------------------------
+        skill_actions = [
+            Config.ACTION_MAPPING["skill_1"],
+            Config.ACTION_MAPPING["skill_2"],
+            Config.ACTION_MAPPING["skill_3"],
+        ]
+        if action[0] in skill_actions:
+            self.pending_normal_attack = True
+        # --------------------------------------------------------
+
+        # 记录动作历史
+        self.action_history.append(action[0])
+        if len(self.action_history) > self.max_history_length:
+            self.action_history.pop(0)
+        
+        return action
+
+    def observation_process(self, observation):
+        """观察处理 - 提取生存相关特征"""
+        obs_data = ObsData(feature=[], legal_action=[], 
+                          lstm_cell=self.lstm_cell, lstm_hidden=self.lstm_hidden)
+        
+        # 提取英雄状态
+        hero_state = self.extract_hero_state(observation)
+        
+        # 添加生存特征
+        survival_features = self.extract_survival_features(hero_state)
+        obs_data.feature.extend(survival_features)
+        
+        # 添加额外属性供策略使用
+        obs_data.hp_ratio = hero_state.get("hp_ratio", 1.0)
+        obs_data.spring_distance = hero_state.get("spring_distance", float('inf'))
+        obs_data.enemy_distance = hero_state.get("enemy_distance", float('inf'))
+        obs_data.spring_direction = hero_state.get("spring_direction", 4)
+        obs_data.enemy_direction = hero_state.get("enemy_direction", 0)
+        obs_data.skill_2_ready = hero_state.get("skill_2_ready", False)
+        
+        # 传递血包相关属性
+        obs_data.pack_available = hero_state.get("pack_available", False)
+        obs_data.pack_distance = hero_state.get("pack_distance", float('inf'))
+        obs_data.pack_direction = hero_state.get("pack_direction", 4)
+        
+        return obs_data
+
+    def extract_hero_state(self, observation):
+        """提取英雄状态信息"""
+        hero_state = {}
+        
+        # 基础血量信息
+        frame_state = observation.get("frame_state", {})
+        hero_states = frame_state.get("hero_states", [])
+        
+        for hero in hero_states:
+            if hero.get("player_id") == self.player_id:
+                actor_state = hero.get("actor_state", {})
+                hp = actor_state.get("hp", 1)
+                max_hp = actor_state.get("max_hp", 1)
+                hero_state["hp_ratio"] = hp / max_hp if max_hp > 0 else 1.0
+                hero_state["position"] = (
+                    actor_state.get("location", {}).get("x", 0),
+                    actor_state.get("location", {}).get("z", 0)
+                )
+                
+                # 技能状态
+                skills = hero.get("skills", [])
+                if len(skills) > 1:
+                    hero_state["skill_2_ready"] = skills[1].get("cd", 0) == 0
+                
+                break
+        
+        # 计算距离信息 (包含血包搜索)
+        hero_state.update(self.calculate_distances(observation, hero_state.get("position")))
+        
+        # 战斗信息
+        hero_state["taking_damage"] = observation.get("taking_damage", False)
+        hero_state["frame_no"] = frame_state.get("frameNo", 0)
+        
+        return hero_state
+
+    def calculate_distances(self, observation, hero_pos):
+        """计算各种距离"""
+        distances = {
+            "spring_distance": float('inf'),
+            "enemy_distance": float('inf'),
+            "tower_distance": float('inf'),
+            "spring_direction": 4,
+            "enemy_direction": 0,
+            # [修改点2] 新增血包信息
+            "pack_distance": float('inf'),
+            "pack_direction": 0,
+            "pack_available": False
+        }
+        
+        if not hero_pos:
+            return distances
+        
+        # 计算到泉水的距离
+        npc_states = observation.get("frame_state", {}).get("npc_states", [])
+        for npc in npc_states:
+            # 泉水
+            if npc.get("sub_type") == "ACTOR_SUB_CRYSTAL" and npc.get("camp") == self.hero_camp:
+                spring_pos = (npc.get("location", {}).get("x", 0),
+                             npc.get("location", {}).get("z", 0))
+                distances["spring_distance"] = self.calculate_distance(hero_pos, spring_pos)
+                distances["spring_direction"] = self.calculate_direction(hero_pos, spring_pos)
+            
+            # [修改点2] 搜索血包 (假设 subtype 为 ACTOR_SUB_HP_MOD 或类似)
+            # 遍历寻找最近的可用血包
+            elif npc.get("sub_type") == "ACTOR_SUB_HP_MOD" and npc.get("camp") == self.hero_camp:
+                # 检查血包状态，假设存在 hp > 0 或 state 字段表示可用
+                # 这里简化为：只要存在即视为可用（具体视环境接口而定）
+                pack_pos = (npc.get("location", {}).get("x", 0),
+                            npc.get("location", {}).get("z", 0))
+                dist = self.calculate_distance(hero_pos, pack_pos)
+                
+                # 如果是最近的血包
+                if dist < distances["pack_distance"]:
+                    distances["pack_distance"] = dist
+                    distances["pack_direction"] = self.calculate_direction(hero_pos, pack_pos)
+                    distances["pack_available"] = True
+
+        # 计算到敌人的距离
+        hero_states = observation.get("frame_state", {}).get("hero_states", [])
+        for hero in hero_states:
+            if hero.get("actor_state", {}).get("camp") != self.hero_camp:
+                enemy_pos = (hero.get("actor_state", {}).get("location", {}).get("x", 0),
+                           hero.get("actor_state", {}).get("location", {}).get("z", 0))
+                distances["enemy_distance"] = self.calculate_distance(hero_pos, enemy_pos)
+                distances["enemy_direction"] = self.calculate_direction(hero_pos, enemy_pos)
+                break
+        
+        return distances
+
+    def calculate_distance(self, pos1, pos2):
+        """计算两点距离"""
+        import math
+        return math.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)
+
+    def calculate_direction(self, from_pos, to_pos):
+        """计算方向（0-7）"""
+        import math
+        dx = to_pos[0] - from_pos[0]
+        dy = to_pos[1] - from_pos[1]
+        angle = math.atan2(dy, dx)
+        direction = int((angle + math.pi) / (math.pi / 4)) % 8
+        return direction
+
+    def extract_survival_features(self, hero_state):
+        """提取生存相关特征"""
+        features = []
+        
+        hp_ratio = hero_state.get("hp_ratio", 1.0)
+        
+        # 血量特征
+        features.append(hp_ratio)
+        features.append(1.0 if hp_ratio < 0.5 else 0.0)
+        features.append(1.0 if hp_ratio < 0.35 else 0.0)
+        features.append(1.0 if hp_ratio < 0.25 else 0.0)
+        
+        # 距离特征（归一化）
+        spring_dist = min(1.0, hero_state.get("spring_distance", 10000) / 10000.0)
+        enemy_dist = min(1.0, hero_state.get("enemy_distance", 10000) / 10000.0)
+        features.extend([spring_dist, enemy_dist])
+        
+        # 战斗状态
+        features.append(1.0 if self.in_combat else 0.0)
+        features.append(1.0 if hero_state.get("taking_damage", False) else 0.0)
+        
+        # 生存状态编码
+        state_encoding = {
+            "normal": 0,
+            "need_retreat": 1,
+            "retreating": 2,
+            "recalling": 3,
+            "recovering": 4
+        }
+        features.append(state_encoding.get(self.survival_state, 0) / 4.0)
+        
+        # 回城可用性
+        features.append(1.0 if self.recall_cooldown == 0 else 0.0)
+        
+        return features
+
+    def update_combat_state(self, hero_state):
+        """更新战斗状态"""
+        enemy_distance = hero_state.get("enemy_distance", float('inf'))
+        taking_damage = hero_state.get("taking_damage", False)
+        
+        # 判断是否在战斗中
+        if enemy_distance < GameConfig.RECALL_CONFIG["combat_distance"] or taking_damage:
+            self.in_combat = True
+            self.combat_frames += 1
+            if taking_damage:
+                self.last_damage_frame = hero_state.get("frame_no", 0)
+        else:
+            # 脱战判定
+            frames_since_damage = hero_state.get("frame_no", 0) - self.last_damage_frame
+            if frames_since_damage > 90:  # 3秒没受伤
+                self.in_combat = False
+                self.combat_frames = 0
+        
+        self.enemy_distance = enemy_distance
+
+    def move_towards_spring(self, action, hero_state):
+        """向泉水移动"""
+        spring_direction = hero_state.get("spring_direction", 4)
+        action[0] = spring_direction
+        return action
+
+    def apply_conservative_strategy(self, action, hero_state):
+        """应用保守策略"""
+        enemy_direction = hero_state.get("enemy_direction", 0)
+        enemy_distance = hero_state.get("enemy_distance", float('inf'))
+        
+        # 保持安全距离
+        if enemy_distance < 4000:
+            # 后退
+            opposite_direction = (enemy_direction + 4) % 8
+            action[0] = opposite_direction
+        
+        return action
+
+    def update_status(self, obs_data, act_data, observation=None):
+        """更新状态"""
+        self.obs_data = obs_data
+        self.act_data = act_data
+        
+        # 更新血量历史
+        if hasattr(obs_data, 'hp_ratio'):
+            self.last_hp_ratio = obs_data.hp_ratio
+        
+        # 更新到泉水的距离
+        if hasattr(obs_data, 'spring_distance'):
+            self.last_distance_to_spring = obs_data.spring_distance
+
+    def reset(self):
+        """重置状态"""
+        self.survival_state = "normal"
+        self.state_counter = 0
+        self.last_hp_ratio = 1.0
+        self.recall_cooldown = 0
+        self.last_recall_frame = 0
+        self.continuous_low_hp_frames = 0
+        self.combat_frames = 0
+        self.in_combat = False
+        self.last_damage_frame = 0
+        self.enemy_distance = float('inf')
+        self.action_history = []
+        # [修改点1] 重置普攻状态
+        self.pending_normal_attack = False
+
     @save_model_wrapper
     def save_model(self, path=None, id="1"):
-        # To save the model, it can consist of multiple files, and it is important to ensure that
-        #  each filename includes the "model.ckpt-id" field.
-        # 保存模型, 可以是多个文件, 需要确保每个文件名里包括了model.ckpt-id字段
+        """保存模型"""
         model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"
         torch.save(self.model.state_dict(), model_file_path)
         self.logger.info(f"save model {model_file_path} successfully")
 
     @load_model_wrapper
     def load_model(self, path=None, id="1"):
-        # When loading the model, you can load multiple files, and it is important to ensure that
-        # each filename matches the one used during the save_model process.
-        # 加载模型, 可以加载多个文件, 注意每个文件名需要和save_model时保持一致
+        """加载模型"""
         model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"
         if self.cur_model_name == model_file_path:
             self.logger.info(f"current model is {model_file_path}, so skip load model")
         else:
             self.model.load_state_dict(
-                torch.load(
-                    model_file_path,
-                    map_location=self.device,
-                )
+                torch.load(model_file_path, map_location=self.device)
             )
             self.cur_model_name = model_file_path
-            self.logger.info(f"load model {model_file_path} successfully")
-
-    def reset(self, hero_camp, player_id):
-        self.hero_camp = hero_camp
-        self.player_id = player_id
-        self.lstm_hidden = np.zeros([self.lstm_unit_size])
-        self.lstm_cell = np.zeros([self.lstm_unit_size])
-        self.reward_manager = GameRewardManager(player_id)
-
-    def update_status(self, obs_data, act_data):
-        self.obs_data = obs_data
-        self.act_data = act_data
-        self.lstm_cell = act_data.lstm_cell
-        self.lstm_hidden = act_data.lstm_hidden
-
-    # get final executable actions
-    def _sample_masked_action(self, logits, legal_action):
-        """
-        Sample actions from predicted logits and legal actions
-        return: probability, stochastic and deterministic actions with additional []
-        """
-        """
-        从预测的logits和合法动作中采样动作
-        返回：以列表形式概率、随机和确定性动作
-        """
-
-        prob_list = []
-        action_list = []
-        d_action_list = []
-        label_split_size = [sum(self.label_size_list[: index + 1]) for index in range(len(self.label_size_list))]
-        legal_actions = np.split(legal_action, label_split_size[:-1])
-        logits_split = np.split(logits, label_split_size[:-1])
-        for index in range(0, len(self.label_size_list) - 1):
-            probs = self._legal_soft_max(logits_split[index], legal_actions[index])
-            prob_list += list(probs)
-            sample_action = self._legal_sample(probs, use_max=False)
-            action_list.append(sample_action)
-            d_action = self._legal_sample(probs, use_max=True)
-            d_action_list.append(d_action)
-
-        # deals with the last prediction, target
-        # 处理最后的预测，目标
-        index = len(self.label_size_list) - 1
-        target_legal_action_o = np.reshape(
-            legal_actions[index],  # [12, 8]
-            [
-                self.legal_action_size[0],
-                self.legal_action_size[-1] // self.legal_action_size[0],
-            ],
-        )
-        one_hot_actions = np.eye(self.label_size_list[0])[action_list[0]]  # [12]
-        one_hot_actions = np.reshape(one_hot_actions, [self.label_size_list[0], 1])  # [12, 1]
-        target_legal_action = np.sum(target_legal_action_o * one_hot_actions, axis=0)
-
-        legal_actions[index] = target_legal_action  # [12]
-        probs = self._legal_soft_max(logits_split[-1], target_legal_action)
-        prob_list += list(probs)
-        sample_action = self._legal_sample(probs, use_max=False)
-        action_list.append(sample_action)
-
-        one_hot_actions = np.eye(self.label_size_list[0])[d_action_list[0]]
-        one_hot_actions = np.reshape(one_hot_actions, [self.label_size_list[0], 1])
-        target_legal_action_d = np.sum(target_legal_action_o * one_hot_actions, axis=0)
-
-        probs = self._legal_soft_max(logits_split[-1], target_legal_action_d)
-
-        d_action = self._legal_sample(probs, use_max=True)
-        d_action_list.append(d_action)
-
-        return [prob_list], action_list, d_action_list
-
-    def _legal_soft_max(self, input_hidden, legal_action):
-        _lsm_const_w, _lsm_const_e = 1e20, 1e-5
-        _lsm_const_e = 0.00001
-
-        tmp = input_hidden - _lsm_const_w * (1.0 - legal_action)
-        tmp_max = np.max(tmp, keepdims=True)
-        tmp = np.clip(tmp - tmp_max, -_lsm_const_w, 1)
-        tmp = (np.exp(tmp) + _lsm_const_e) * legal_action
-        probs = tmp / np.sum(tmp, keepdims=True)
-        return probs
-
-    def _legal_sample(self, probs, legal_action=None, use_max=False):
-        # Sample with probability, input probs should be 1D array
-        # 根据概率采样，输入的probs应该是一维数组
-        if use_max:
-            return np.argmax(probs)
-
-        return np.argmax(np.random.multinomial(1, probs, size=1))
-
-    def load_model_local(self, model_file_path, idx):
-        # When loading the local model, you can load multiple files, and it is important to ensure that
-        # each filename matches the one used during the save_model process.
-        # 加载本地模型, 可以加载多个文件, 注意每个文件名需要和save_model时保持一致
-        self.is_predict_remote = False
-        self.model.load_state_dict(
-            torch.load(
-                model_file_path,
-                map_location=torch.device("cpu"),
-            )
-        )
-        self.logger.info(f"agent {idx} load model {model_file_path} successfully")
+            self.logger.info(f"load model {model_file_path} successfully")
\ No newline at end of file
diff --git a/agent_ppo/algorithm/algorithm.py b/agent_ppo/algorithm/algorithm.py
index 3d3d556..8267036 100644
--- a/agent_ppo/algorithm/algorithm.py
+++ b/agent_ppo/algorithm/algorithm.py
@@ -1,19 +1,11 @@
 #!/usr/bin/env python3
 # -*- coding: UTF-8 -*-
-###########################################################################
-# Copyright © 1998 - 2025 Tencent. All Rights Reserved.
-###########################################################################
-"""
-Author: Tencent AI Arena Authors
-"""
-
 
 import torch
 import numpy as np
 import os
 import time
-from agent_ppo.conf.conf import Config
-
+from agent_diy.conf.conf import Config
 
 class Algorithm:
     def __init__(self, model, optimizer, scheduler, device=None, logger=None, monitor=None):
@@ -33,6 +25,21 @@ class Algorithm:
         self.lstm_unit_size = Config.LSTM_UNIT_SIZE
 
         self.last_report_monitor_time = 0
+        
+        # 新增：奖励监控统计
+        self.reward_stats = {
+            "damage_exchange": [],
+            "tower_defense": [],
+            "skill_hit": [],
+            "safe_positioning": [],
+            "forward": [],
+            "kill": [],
+            "death": [],
+            "tower_hp_point": [],
+            # [修改点] 新增小兵伤害奖励监控，验证清线策略
+            "minion_damage": [] 
+        }
+        self.stats_window_size = 100  # 统计窗口大小
 
     def learn(self, list_sample_data):
         _input_datas = torch.stack([t.npdata for t in list_sample_data], dim=0)
@@ -69,15 +76,13 @@ class Algorithm:
 
         total_loss.backward()
 
-        # grad clip
-        # 梯度剪裁
+        # 梯度裁剪
         if Config.USE_GRAD_CLIP:
             torch.nn.utils.clip_grad_norm_(self.parameters, Config.GRAD_CLIP_RANGE)
 
         self.optimizer.step()
         self.train_step += 1
 
-        # update the learning rate
         # 更新学习率
         self.scheduler.step(self.train_step)
 
@@ -89,12 +94,80 @@ class Algorithm:
                 _info = info.item()
             _info_list.append(_info)
 
+        # 增强的监控输出
         now = time.time()
         if now - self.last_report_monitor_time >= 60:
             _, (value_loss, policy_loss, entropy_loss) = _info_list
             results["value_loss"] = round(value_loss, 2)
             results["policy_loss"] = round(policy_loss, 2)
             results["entropy_loss"] = round(entropy_loss, 2)
+            
+            # 添加奖励统计监控
+            results.update(self.get_reward_statistics())
+            
             if self.monitor:
                 self.monitor.put_data({os.getpid(): results})
             self.last_report_monitor_time = now
+            
+            # 打印详细日志
+            if self.logger:
+                self.log_training_status(results)
+                
+        return results
+
+    def update_reward_stats(self, reward_dict):
+        """更新奖励统计数据"""
+        for key in self.reward_stats:
+            if key in reward_dict:
+                self.reward_stats[key].append(reward_dict[key])
+                # 保持窗口大小
+                if len(self.reward_stats[key]) > self.stats_window_size:
+                    self.reward_stats[key].pop(0)
+
+    def get_reward_statistics(self):
+        """计算奖励统计信息"""
+        stats = {}
+        for key, values in self.reward_stats.items():
+            if values:
+                stats[f"{key}_mean"] = round(np.mean(values), 4)
+                stats[f"{key}_std"] = round(np.std(values), 4)
+                stats[f"{key}_max"] = round(np.max(values), 4)
+        return stats
+
+    def log_training_status(self, results):
+        """详细的训练状态日志"""
+        self.logger.info(f"Training Step: {self.train_step}")
+        self.logger.info(f"Losses - Total: {results.get('total_loss', 0):.4f}, "
+                        f"Value: {results.get('value_loss', 0):.4f}, "
+                        f"Policy: {results.get('policy_loss', 0):.4f}, "
+                        f"Entropy: {results.get('entropy_loss', 0):.4f}")
+        
+        # 记录关键奖励指标
+        # [修改点] 添加 minion_damage_mean 到日志中
+        key_rewards = ["damage_exchange_mean", "tower_defense_mean", 
+                      "skill_hit_mean", "forward_mean", "minion_damage_mean"]
+        reward_info = []
+        for key in key_rewards:
+            if key in results:
+                reward_info.append(f"{key.replace('_mean', '')}: {results[key]:.4f}")
+        
+        if reward_info:
+            self.logger.info(f"Key Rewards - {', '.join(reward_info)}")
+        
+        # 学习率
+        current_lr = self.scheduler.get_last_lr()[0] if hasattr(self.scheduler, 'get_last_lr') else 0
+        self.logger.info(f"Learning Rate: {current_lr:.6f}")
+
+    def should_save_model(self):
+        """判断是否应该保存模型（基于性能改进）"""
+        # 可以基于奖励统计来判断模型是否改进
+        if not hasattr(self, 'best_reward_sum'):
+            self.best_reward_sum = -float('inf')
+        
+        current_reward_sum = sum([np.mean(v) for v in self.reward_stats.values() if v])
+        
+        if current_reward_sum > self.best_reward_sum * 1.05:  # 5%的改进
+            self.best_reward_sum = current_reward_sum
+            return True
+        
+        return False
\ No newline at end of file
diff --git a/agent_ppo/conf/conf.py b/agent_ppo/conf/conf.py
index 004859a..acceac3 100644
--- a/agent_ppo/conf/conf.py
+++ b/agent_ppo/conf/conf.py
@@ -1,37 +1,83 @@
 #!/usr/bin/env python3
 # -*- coding: UTF-8 -*-
-###########################################################################
-# Copyright © 1998 - 2025 Tencent. All Rights Reserved.
-###########################################################################
-"""
-Author: Tencent AI Arena Authors
-"""
-
 
 class GameConfig:
-    # Set the weight of each reward item and use it in reward_manager
-    # 设置各个回报项的权重，在reward_manager中使用
+    # 改进的奖励权重设计 - 完整生存策略系统
     REWARD_WEIGHT_DICT = {
-        "hp_point": 2.0,
-        "tower_hp_point": 5.0,
-        "money": 0.006,
-        "exp": 0.006,
-        "ep_rate": 0.75,
-        "death": -1.0,
-        "kill": -0.6,
-        "last_hit": 0.5,
-        "forward": 0.01,
+        # ===== 基础奖励 =====
+        "hp_point": 2.5,            # 降低，避免过于保守
+        "money": 0.1,               # 金币奖励
+        "exp": 0.1,                 # 经验奖励
+        "last_hit": 2.5,            # 补刀奖励提升
+        
+        # [修改点2] 新增：对小兵造成伤害奖励 (鼓励清线)
+        "minion_damage": 0.15,
+        
+        # ===== 战斗奖励 =====
+        "kill": 6.0,                # 击杀奖励
+        "death": -5.0,              # 死亡惩罚
+        "ep_rate": -0.2,            # 能量管理
+        
+        # ===== 推进奖励 =====
+        "forward": 0.5,             
+        "tower_hp_point": 12.0,     # 塔血保护
+        
+        # ===== 密集奖励系统 =====
+        "damage_exchange": 0.5,      # 伤害交换效率
+        "skill_hit": 1.5,           # 技能命中
+        
+        # ===== 生存策略奖励（核心） =====
+        "recall_decision": 4.0,      # 回城决策
+        "health_management": 2.0,    # 血量管理
+        "combat_timing": 1.5,       # 战斗时机选择
+        "tower_defense": 3.0,       # 塔防奖励
+        "positioning": 1.0,         # 位置奖励
+    }
+    
+    # 动态权重调整配置
+    ADAPTIVE_WEIGHT = {
+        "early_game": {  # 前5000帧
+            "money": 1.5,
+            "exp": 1.5,
+            "death": 0.7,           # 早期降低死亡惩罚
+            "recall_decision": 1.5,  # 早期强化生存意识
+            "health_management": 1.5,
+        },
+        "mid_game": {    # 5000-10000帧
+            "tower_defense": 1.5,
+            "damage_exchange": 1.5,
+            "combat_timing": 1.5,
+        },
+        "late_game": {   # 10000帧后
+            "tower_hp_point": 8.0,
+            "forward": 5.0,
+            "kill": 1.5,
+        }
+    }
+    
+    # 回城配置
+    RECALL_CONFIG = {
+        "low_hp_threshold": 0.30,        # 需要考虑回城的血量
+        "critical_hp_threshold": 0.1 ,   # 必须回城的血量
+        "safe_hp_threshold": 0.7,        # 安全血量
+        "spring_distance": 2000,         # 泉水判定距离
+        "combat_distance": 6000,         # 战斗距离判定
+        "retreat_frames": 30,            # 撤退评估帧数
     }
-    # Time decay factor, used in reward_manager
-    # 时间衰减因子，在reward_manager中使用
+    
+    # 战斗配置
+    COMBAT_CONFIG = {
+        "ideal_distance_aggressive": 4000,  # 激进距离
+        "ideal_distance_normal": 5500,      # 正常对线距离
+        "ideal_distance_defensive": 8000,   # 防守距离
+        "hp_advantage_threshold": 0.3,      # 血量优势阈值
+        "hp_disadvantage_threshold": -0.2,  # 血量劣势阈值
+    }
+    
     TIME_SCALE_ARG = 0
-    # Model save interval configuration, used in workflow
-    # 模型保存间隔配置，在workflow中使用
     MODEL_SAVE_INTERVAL = 1800
 
-
-# Dimension configuration, used when building the model
-# 维度配置，构建模型时使用
+# 维度配置
 class DimConfig:
     DIM_OF_SOLDIER_1_10 = [18, 18, 18, 18]
     DIM_OF_SOLDIER_11_20 = [18, 18, 18, 18]
@@ -42,9 +88,7 @@ class DimConfig:
     DIM_OF_HERO_MAIN = [14]
     DIM_OF_GLOBAL_INFO = [25]
 
-
-# Configuration related to model and algorithms used
-# 模型和算法使用的相关配置
+# 模型和算法配置
 class Config:
     NETWORK_NAME = "network"
     LSTM_TIME_STEPS = 16
@@ -76,9 +120,12 @@ class Config:
         LSTM_UNIT_SIZE,
     ]
     SERI_VEC_SPLIT_SHAPE = [(725,), (85,)]
-    INIT_LEARNING_RATE_START = 1e-3
-    TARGET_LR = 1e-4
+    
+    # 学习率配置（略微调整）
+    INIT_LEARNING_RATE_START = 8e-4  # 略微降低学习率
+    TARGET_LR = 8e-5
     TARGET_STEP = 5000
+    
     BETA_START = 0.025
     LOG_EPSILON = 1e-6
     LABEL_SIZE_LIST = [12, 16, 16, 16, 16, 9]
@@ -92,9 +139,11 @@ class Config:
     ]
 
     CLIP_PARAM = 0.2
-
+    
+    # ===== Dual-Clip参数 =====
+    DUAL_CLIP_PARAM_C = 0.75  # 从3.0降低到0.75，平衡探索与利用
+    
     MIN_POLICY = 0.00001
-
     TARGET_EMBED_DIM = 32
 
     data_shapes = [
@@ -133,8 +182,23 @@ class Config:
     USE_GRAD_CLIP = True
     GRAD_CLIP_RANGE = 0.5
 
-    # The input dimension of samples on the learner from Reverb varies depending on the algorithm used.
-    # For instance, the dimension for ppo is 15584,
-    # learner上reverb样本的输入维度, 注意不同的算法维度不一样, 比如示例代码中ppo的维度是15584
-    # **注意**，此项必须正确配置，应该与definition.py中的NumpyData2SampleData函数数据对齐，否则可能报样本维度错误
     SAMPLE_DIM = sum(DATA_SPLIT_SHAPE[:-2]) * LSTM_TIME_STEPS + sum(DATA_SPLIT_SHAPE[-2:])
+    
+    # 动作映射（鲁班7号相关）
+    ACTION_MAPPING = {
+        "move": [0, 1, 2, 3, 4, 5, 6, 7, 8],  # 移动方向
+        "attack": 9,        # 普攻
+        "skill_1": 10,      # 技能1
+        "skill_2": 11,      # 技能2
+        "skill_3": 12,      # 技能3（大招）
+        "recall": 13,       # 回城
+        "no_op": 14,        # 不操作
+    }
+    
+    # 鲁班技能配置
+    HERO_SKILL_CONFIG = {
+        "skill_1_range": 5000,   # 技能1射程
+        "skill_2_range": 6000,   # 技能2射程
+        "skill_3_range": 8000,   # 大招射程
+        "normal_attack_range": 4500,  # 普攻距离
+    }
\ No newline at end of file
diff --git a/agent_ppo/feature/definition.py b/agent_ppo/feature/definition.py
index a54e610..1e38b4b 100644
--- a/agent_ppo/feature/definition.py
+++ b/agent_ppo/feature/definition.py
@@ -11,7 +11,7 @@ Author: Tencent AI Arena Authors
 from kaiwu_agent.utils.common_func import create_cls, Frame, attached
 import numpy as np
 import collections
-from agent_ppo.conf.conf import Config
+from agent_diy.conf.conf import Config
 
 
 SampleData = create_cls("SampleData", npdata=None)
diff --git a/agent_ppo/feature/reward_process.py b/agent_ppo/feature/reward_process.py
index 8c0547b..91c43b0 100644
--- a/agent_ppo/feature/reward_process.py
+++ b/agent_ppo/feature/reward_process.py
@@ -1,19 +1,10 @@
 #!/usr/bin/env python3
 # -*- coding: UTF-8 -*-
-###########################################################################
-# Copyright © 1998 - 2025 Tencent. All Rights Reserved.
-###########################################################################
-"""
-Author: Tencent AI Arena Authors
-"""
-
 
 import math
-from agent_ppo.conf.conf import GameConfig
-
+import numpy as np
+from agent_diy.conf.conf import GameConfig
 
-# Used to record various reward information
-# 用于记录各个奖励信息
 class RewardStruct:
     def __init__(self, m_weight=0.0):
         self.cur_frame_value = 0.0
@@ -23,16 +14,12 @@ class RewardStruct:
         self.min_value = -1
         self.is_first_arrive_center = True
 
-
-# Used to initialize various reward information
-# 用于初始化各个奖励信息
 def init_calc_frame_map():
     calc_frame_map = {}
     for key, weight in GameConfig.REWARD_WEIGHT_DICT.items():
         calc_frame_map[key] = RewardStruct(weight)
     return calc_frame_map
 
-
 class GameRewardManager:
     def __init__(self, main_hero_runtime_id):
         self.main_hero_player_id = main_hero_runtime_id
@@ -48,44 +35,406 @@ class GameRewardManager:
         self.time_scale_arg = GameConfig.TIME_SCALE_ARG
         self.m_main_hero_config_id = -1
         self.m_each_level_max_exp = {}
+        
+        # 历史数据追踪
+        self.last_hero_hp = 1.0
+        self.last_enemy_hp = 1.0
+        self.last_tower_hp = 1.0
+        self.last_hero_pos = None
+        self.skill_cooldowns = [0, 0, 0]
+        
+        # [修改点2] 小兵血量追踪
+        self.last_enemy_minion_hp = {}
+        
+        # 生存状态机
+        self.survival_state = "normal"  # normal, need_retreat, retreating, recalling, recovering
+        self.state_counter = 0
+        self.last_hp_ratio = 1.0
+        self.recall_cooldown = 0
+        self.combat_frames = 0
+        self.last_damage_taken = 0
+        self.last_distance_to_spring = float('inf')
 
-    # Used to initialize the maximum experience value for each agent level
-    # 用于初始化智能体各个等级的最大经验值
     def init_max_exp_of_each_hero(self):
         self.m_each_level_max_exp.clear()
-        self.m_each_level_max_exp[1] = 160
-        self.m_each_level_max_exp[2] = 298
-        self.m_each_level_max_exp[3] = 446
-        self.m_each_level_max_exp[4] = 524
-        self.m_each_level_max_exp[5] = 613
-        self.m_each_level_max_exp[6] = 713
-        self.m_each_level_max_exp[7] = 825
-        self.m_each_level_max_exp[8] = 950
-        self.m_each_level_max_exp[9] = 1088
-        self.m_each_level_max_exp[10] = 1240
-        self.m_each_level_max_exp[11] = 1406
-        self.m_each_level_max_exp[12] = 1585
-        self.m_each_level_max_exp[13] = 1778
-        self.m_each_level_max_exp[14] = 1984
+        exp_levels = [160, 298, 446, 524, 613, 713, 825, 950, 1088, 1240, 1406, 1585, 1778, 1984]
+        for i, exp in enumerate(exp_levels, 1):
+            self.m_each_level_max_exp[i] = exp
 
     def result(self, frame_data):
         self.init_max_exp_of_each_hero()
         self.frame_data_process(frame_data)
         self.get_reward(frame_data, self.m_reward_value)
-
+        
         frame_no = frame_data["frameNo"]
+        
+        # 动态权重调整
+        weight_multiplier = self.get_adaptive_weight_multiplier(frame_no)
+        for key in self.m_reward_value:
+            if key in weight_multiplier:
+                self.m_reward_value[key] *= weight_multiplier[key]
+        
+        # 时间衰减
         if self.time_scale_arg > 0:
             for key in self.m_reward_value:
                 self.m_reward_value[key] *= math.pow(0.6, 1.0 * frame_no / self.time_scale_arg)
-
+        
         return self.m_reward_value
 
-    # Calculate the value of each reward item in each frame
-    # 计算每帧的每个奖励子项的信息
-    def set_cur_calc_frame_vec(self, cul_calc_frame_map, frame_data, camp):
+    def get_adaptive_weight_multiplier(self, frame_no):
+        """根据游戏阶段动态调整权重"""
+        weight_multiplier = {}
+        
+        if frame_no < 5000:  # 早期
+            weight_multiplier = GameConfig.ADAPTIVE_WEIGHT.get("early_game", {})
+        elif frame_no < 10000:  # 中期
+            weight_multiplier = GameConfig.ADAPTIVE_WEIGHT.get("mid_game", {})
+        else:  # 后期
+            weight_multiplier = GameConfig.ADAPTIVE_WEIGHT.get("late_game", {})
+        
+        return weight_multiplier
+
+    # [修改点4] 增加 frame_no 参数
+    def calculate_recall_reward(self, main_hero, main_spring, enemy_hero, frame_no):
+        """计算回城奖励 - 核心生存机制"""
+        if not main_spring:
+            return 0.0
+        
+        hero_hp = main_hero["actor_state"]["hp"]
+        hero_max_hp = main_hero["actor_state"]["max_hp"]
+        hero_hp_ratio = hero_hp / hero_max_hp if hero_max_hp > 0 else 1.0
+        
+        hero_pos = (main_hero["actor_state"]["location"]["x"],
+                   main_hero["actor_state"]["location"]["z"])
+        spring_pos = (main_spring["location"]["x"],
+                     main_spring["location"]["z"])
+        
+        distance_to_spring = math.dist(hero_pos, spring_pos)
+        
+        # 更新回城冷却
+        if self.recall_cooldown > 0:
+            self.recall_cooldown -= 1
+        
+        reward = 0.0
+        
+        # 检查是否在战斗中
+        in_combat = False
+        if enemy_hero:
+            enemy_pos = (enemy_hero["actor_state"]["location"]["x"],
+                        enemy_hero["actor_state"]["location"]["z"])
+            enemy_distance = math.dist(hero_pos, enemy_pos)
+            in_combat = enemy_distance < 6000
+        
+        # 动态获取阈值 (确保使用修改后的 0.30)
+        low_hp_threshold = GameConfig.RECALL_CONFIG["low_hp_threshold"]
+
+        # 状态机逻辑
+        if self.survival_state == "normal":
+            # 正常状态 -> 检查是否需要撤退
+            if hero_hp_ratio < low_hp_threshold:  # [修改点4] 使用 0.30 阈值
+                self.survival_state = "need_retreat"
+                self.state_counter = 0
+                reward += 0.5  # 识别危险的小奖励
+                
+        elif self.survival_state == "need_retreat":
+            # 需要撤退状态
+            self.state_counter += 1
+            
+            if hero_hp_ratio < 0.25:  # 血量持续下降到25%
+                if in_combat:
+                    # 战斗中低血量，强烈惩罚
+                    reward -= 3.0
+                else:
+                    # 脱战了，应该回城
+                    if distance_to_spring > 1000:
+                        reward -= 2.0  # 惩罚不回城
+                    else:
+                        self.survival_state = "recovering"
+                        reward += 3.0  # 奖励成功回城
+            
+            # 检查是否正在撤退
+            if distance_to_spring < self.last_distance_to_spring:
+                reward += 1.0  # 奖励向泉水移动
+                self.survival_state = "retreating"
+                
+        elif self.survival_state == "retreating":
+            # 撤退中
+            if distance_to_spring < 2000:  # 接近泉水
+                self.survival_state = "recovering"
+                reward += 2.5  # 奖励成功撤退到安全区
+            elif in_combat:
+                reward -= 1.0  # 撤退中被追击
+                
+        elif self.survival_state == "recovering":
+            # 恢复中
+            self.state_counter += 1 # 计数
+
+            # [修改点4] 后期取消血量比逻辑
+            is_late_game = frame_no >= 10000 
+            
+            if is_late_game:
+                 # 后期：只要待够时间 (e.g. 50帧) 即算成功，不强求回满血
+                 if self.state_counter >= 50:
+                    self.survival_state = "normal"
+                    reward += 4.0
+                    self.recall_cooldown = 300
+                 elif distance_to_spring > 2000:
+                    reward -= 1.5
+            else:
+                # 前中期：需要回血
+                if hero_hp_ratio > 0.8:  # 血量恢复到80%
+                    self.survival_state = "normal"
+                    reward += 4.0  # 奖励完成整个生存循环
+                    self.recall_cooldown = 300  # 设置回城冷却
+                elif distance_to_spring > 2000:  # 恢复未完成就离开
+                    reward -= 1.5  # 惩罚过早离开泉水
+        
+        # 存储距离用于下一帧比较
+        self.last_distance_to_spring = distance_to_spring
+        
+        # 额外奖励：满血不要待在泉水
+        if hero_hp_ratio > 0.9 and distance_to_spring < 1000:
+            reward -= 0.8  # 满血赖泉水的惩罚
+        
+        return reward
+    
+    # [修改点2] 新增函数：计算小兵伤害奖励
+    def calculate_minion_damage_reward(self, frame_data, main_camp):
+        """计算对敌方小兵造成伤害的奖励"""
+        reward = 0.0
+        
+        npc_list = frame_data["npc_states"]
+        
+        for npc in npc_list:
+            if npc["camp"] != main_camp and npc["sub_type"] == "ACTOR_SUB_SOLDIER":
+                minion_id = npc["runtime_id"]
+                current_hp = npc["hp"]
+                
+                if minion_id in self.last_enemy_minion_hp:
+                    # 造成伤害 = 上一帧血量 - 当前血量
+                    damage_dealt = max(0, self.last_enemy_minion_hp[minion_id] - current_hp)
+                    
+                    # 简单奖励：每100点伤害给 0.01 奖励 (基础值, 会被权重放大)
+                    reward += damage_dealt / 1000.0 
+                
+                self.last_enemy_minion_hp[minion_id] = current_hp
+                
+        # 清理已阵亡小兵的记录
+        current_minion_ids = {npc["runtime_id"] for npc in npc_list if npc["sub_type"] == "ACTOR_SUB_SOLDIER" and npc["camp"] != main_camp}
+        self.last_enemy_minion_hp = {k: v for k, v in self.last_enemy_minion_hp.items() if k in current_minion_ids}
+        
+        return reward
+
+    def calculate_damage_exchange_reward(self, main_hero, enemy_hero):
+        """计算即时伤害交换奖励"""
+        if not enemy_hero:
+            return 0.0
+            
+        if not hasattr(self, 'last_main_hp'):
+            self.last_main_hp = main_hero["actor_state"]["hp"]
+            self.last_enemy_hp = enemy_hero["actor_state"]["hp"]
+            return 0.0
+        
+        # 计算本帧伤害
+        damage_dealt = max(0, self.last_enemy_hp - enemy_hero["actor_state"]["hp"])
+        damage_taken = max(0, self.last_main_hp - main_hero["actor_state"]["hp"])
+        
+        # 更新历史血量
+        self.last_main_hp = main_hero["actor_state"]["hp"]
+        self.last_enemy_hp = enemy_hero["actor_state"]["hp"]
+        self.last_damage_taken = damage_taken
+        
+        # 血量比例
+        hero_hp_ratio = main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"]
+        enemy_hp_ratio = enemy_hero["actor_state"]["hp"] / enemy_hero["actor_state"]["max_hp"]
+        
+        # 基础交换比
+        if damage_dealt + damage_taken > 0:
+            exchange_ratio = (damage_dealt - damage_taken) / 100.0
+        else:
+            exchange_ratio = 0
+        
+        # 根据双方血量调整奖励
+        hp_advantage = hero_hp_ratio - enemy_hp_ratio
+        
+        reward = 0.0
+        
+        # 低血量决策
+        if hero_hp_ratio < 0.3:
+            if damage_taken > 0:
+                reward -= damage_taken * 0.02  # 低血量受伤严重惩罚
+            elif damage_dealt > 0 and damage_taken == 0:
+                reward += damage_dealt * 0.01  # 安全输出奖励
+        
+        # 血量优势时的交换
+        elif hp_advantage > 0.2:
+            reward += exchange_ratio * 0.3  # 有优势时鼓励换血
+        
+        # 血量劣势时的交换
+        elif hp_advantage < -0.2:
+            if exchange_ratio > 0:
+                reward += exchange_ratio * 0.5  # 劣势但打出优势交换，额外奖励
+            else:
+                reward += exchange_ratio * 0.1  # 劣势换血失败，轻微惩罚
+        
+        # 均势交换
+        else:
+            reward += exchange_ratio * 0.2
+        
+        return reward
 
-        # Get both agents
-        # 获取双方智能体
+    def calculate_health_management_reward(self, main_hero):
+        """血量管理奖励"""
+        hero_hp_ratio = main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"]
+        
+        reward = 0.0
+        
+        # 维持健康血线奖励
+        if 0.5 < hero_hp_ratio < 0.8:
+            reward += 0.3  # 健康血量区间
+        elif hero_hp_ratio > 0.8:
+            reward += 0.5  # 高血量奖励
+        elif hero_hp_ratio < 0.3:
+            reward -= 0.5  # 危险血量惩罚
+        
+        # 血量回复奖励
+        if hasattr(self, 'last_hp_ratio'):
+            hp_change = hero_hp_ratio - self.last_hp_ratio
+            if hp_change > 0 and self.last_hp_ratio < 0.5:
+                reward += hp_change * 2.0  # 低血量回复奖励
+        
+        self.last_hp_ratio = hero_hp_ratio
+        
+        return reward
+
+    def calculate_combat_timing_reward(self, main_hero, enemy_hero):
+        """战斗时机选择奖励"""
+        if not enemy_hero:
+            return 0.0
+            
+        hero_hp_ratio = main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"]
+        enemy_hp_ratio = enemy_hero["actor_state"]["hp"] / enemy_hero["actor_state"]["max_hp"]
+        
+        hero_pos = (main_hero["actor_state"]["location"]["x"],
+                   main_hero["actor_state"]["location"]["z"])
+        enemy_pos = (enemy_hero["actor_state"]["location"]["x"],
+                    enemy_hero["actor_state"]["location"]["z"])
+        
+        distance = math.dist(hero_pos, enemy_pos)
+        
+        reward = 0.0
+        
+        # 进入战斗距离
+        if distance < 5000:
+            self.combat_frames += 1
+            
+            # 有利时机进攻
+            if hero_hp_ratio > enemy_hp_ratio * 1.3:
+                reward += 0.5  # 血量优势时进攻
+            
+            # 不利时机进攻惩罚
+            elif hero_hp_ratio < enemy_hp_ratio * 0.7:
+                reward -= 1.0  # 血量劣势还要打
+            
+            # 持续战斗时长控制
+            if self.combat_frames > 100:
+                if hero_hp_ratio < 0.5:
+                    reward -= 0.5  # 长时间战斗且血量不健康
+        else:
+            self.combat_frames = 0
+        
+        return reward
+
+    def calculate_tower_defense_reward(self, main_hero, main_tower, enemy_hero):
+        """塔防奖励"""
+        if not main_tower:
+            return 0.0
+        
+        hero_pos = (main_hero["actor_state"]["location"]["x"],
+                   main_hero["actor_state"]["location"]["z"])
+        tower_pos = (main_tower["location"]["x"],
+                    main_tower["location"]["z"])
+        
+        distance_to_tower = math.dist(hero_pos, tower_pos)
+        tower_hp_ratio = main_tower["hp"] / main_tower["max_hp"]
+        
+        # 危险等级
+        danger_level = (1 - tower_hp_ratio) ** 2
+        
+        reward = 0.0
+        
+        # 塔受威胁时
+        if tower_hp_ratio < 0.7:
+            if enemy_hero:
+                enemy_pos = (enemy_hero["actor_state"]["location"]["x"],
+                           enemy_hero["actor_state"]["location"]["z"])
+                enemy_to_tower = math.dist(enemy_pos, tower_pos)
+                
+                if enemy_to_tower < 6000:  # 敌人威胁塔
+                    if distance_to_tower < 5000:
+                        reward += danger_level * 1.5  # 守护塔
+                    else:
+                        reward -= danger_level * 2.0  # 塔被攻击但不在
+        
+        return reward
+
+    def calculate_skill_hit_reward(self, frame_action):
+        """技能命中奖励"""
+        reward = 0.0
+        
+        if "skill_action" not in frame_action:
+            return 0.0
+        
+        skill_actions = frame_action.get("skill_action", [])
+        for skill in skill_actions:
+            if skill.get("hit", False):
+                skill_type = skill.get("skill_type", 1)
+                base_reward = [0.3, 0.5, 0.8][min(skill_type - 1, 2)]
+                
+                target_hp_ratio = skill.get("target_hp_ratio", 1.0)
+                if target_hp_ratio < 0.3:
+                    base_reward *= 1.5
+                
+                reward += base_reward
+        
+        return reward
+
+    def calculate_positioning_reward(self, main_hero, enemy_hero, main_tower):
+        """位置奖励"""
+        if not enemy_hero:
+            return 0.0
+            
+        hero_pos = (main_hero["actor_state"]["location"]["x"],
+                   main_hero["actor_state"]["location"]["z"])
+        enemy_pos = (enemy_hero["actor_state"]["location"]["x"],
+                    enemy_hero["actor_state"]["location"]["z"])
+        
+        distance = math.dist(hero_pos, enemy_pos)
+        hero_hp_ratio = main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"]
+        enemy_hp_ratio = enemy_hero["actor_state"]["hp"] / enemy_hero["actor_state"]["max_hp"]
+        
+        reward = 0.0
+        
+        # 根据血量调整理想距离
+        if hero_hp_ratio < 0.3:
+            ideal_distance = 8000  # 低血量保持距离
+            if distance < 5000:
+                reward -= 1.0  # 太近了
+        elif hero_hp_ratio > enemy_hp_ratio * 1.5:
+            ideal_distance = 4000  # 优势可以压进
+            if distance > 7000:
+                reward -= 0.3  # 太保守
+        else:
+            ideal_distance = 5500  # 正常对线距离
+        
+        # 距离偏差惩罚
+        distance_diff = abs(distance - ideal_distance) / ideal_distance
+        reward += max(0, 1 - distance_diff) * 0.2
+        
+        return reward
+
+    def set_cur_calc_frame_vec(self, cul_calc_frame_map, frame_data, camp):
+        # 获取英雄和防御塔
         main_hero, enemy_hero = None, None
         hero_list = frame_data["hero_states"]
         for hero in hero_list:
@@ -94,13 +443,16 @@ class GameRewardManager:
                 main_hero = hero
             else:
                 enemy_hero = hero
+        
+        if not main_hero:
+            return
+            
         main_hero_hp = main_hero["actor_state"]["hp"]
         main_hero_max_hp = main_hero["actor_state"]["max_hp"]
         main_hero_ep = main_hero["actor_state"]["values"]["ep"]
         main_hero_max_ep = main_hero["actor_state"]["values"]["max_ep"]
 
-        # Get both defense towers
-        # 获取双方防御塔
+        # 获取防御塔和泉水
         main_tower, main_spring, enemy_tower, enemy_spring = None, None, None, None
         npc_list = frame_data["npc_states"]
         for organ in npc_list:
@@ -117,64 +469,76 @@ class GameRewardManager:
                 elif organ_subtype == "ACTOR_SUB_CRYSTAL":
                     enemy_spring = organ
 
+        frame_no = frame_data["frameNo"]
+
+        # 计算所有奖励
         for reward_name, reward_struct in cul_calc_frame_map.items():
             reward_struct.last_frame_value = reward_struct.cur_frame_value
-            # Money
-            # 金钱
+            
+            # 基础奖励
             if reward_name == "money":
                 reward_struct.cur_frame_value = main_hero["moneyCnt"]
-            # Health points
-            # 生命值
             elif reward_name == "hp_point":
                 reward_struct.cur_frame_value = math.sqrt(math.sqrt(1.0 * main_hero_hp / main_hero_max_hp))
-            # Energy points
-            # 法力值
             elif reward_name == "ep_rate":
                 if main_hero_max_ep == 0 or main_hero_hp <= 0:
                     reward_struct.cur_frame_value = 0
                 else:
                     reward_struct.cur_frame_value = main_hero_ep / float(main_hero_max_ep)
-            # Kills
-            # 击杀
             elif reward_name == "kill":
                 reward_struct.cur_frame_value = main_hero["killCnt"]
-            # Deaths
-            # 死亡
             elif reward_name == "death":
                 reward_struct.cur_frame_value = main_hero["deadCnt"]
-            # Tower health points
-            # 塔血量
             elif reward_name == "tower_hp_point":
-                reward_struct.cur_frame_value = 1.0 * main_tower["hp"] / main_tower["max_hp"]
-            # Last hit
-            # 补刀
+                if main_tower:
+                    reward_struct.cur_frame_value = 1.0 * main_tower["hp"] / main_tower["max_hp"]
+                else:
+                    reward_struct.cur_frame_value = 0
             elif reward_name == "last_hit":
                 reward_struct.cur_frame_value = 0.0
                 frame_action = frame_data["frame_action"]
                 if "dead_action" in frame_action:
                     dead_actions = frame_action["dead_action"]
                     for dead_action in dead_actions:
-                        if (
-                            dead_action["killer"]["runtime_id"] == main_hero["actor_state"]["runtime_id"]
-                            and dead_action["death"]["sub_type"] == "ACTOR_SUB_SOLDIER"
-                        ):
+                        if (dead_action["killer"]["runtime_id"] == main_hero["actor_state"]["runtime_id"]
+                            and dead_action["death"]["sub_type"] == "ACTOR_SUB_SOLDIER"):
                             reward_struct.cur_frame_value += 1.0
-                        elif (
-                            dead_action["killer"]["runtime_id"] == enemy_hero["actor_state"]["runtime_id"]
-                            and dead_action["death"]["sub_type"] == "ACTOR_SUB_SOLDIER"
-                        ):
+                        elif enemy_hero and (dead_action["killer"]["runtime_id"] == enemy_hero["actor_state"]["runtime_id"]
+                              and dead_action["death"]["sub_type"] == "ACTOR_SUB_SOLDIER"):
                             reward_struct.cur_frame_value -= 1.0
-            # Experience points
-            # 经验值
             elif reward_name == "exp":
                 reward_struct.cur_frame_value = self.calculate_exp_sum(main_hero)
-            # Forward
-            # 前进
             elif reward_name == "forward":
-                reward_struct.cur_frame_value = self.calculate_forward(main_hero, main_tower, enemy_tower)
+                reward_struct.cur_frame_value = self.calculate_forward_improved(
+                    main_hero, main_tower, enemy_tower)
+            
+            # 新增密集奖励
+            elif reward_name == "damage_exchange":
+                reward_struct.cur_frame_value = self.calculate_damage_exchange_reward(
+                    main_hero, enemy_hero)
+            elif reward_name == "recall_decision":
+                # [修改点4] 传递 frame_no
+                reward_struct.cur_frame_value = self.calculate_recall_reward(
+                    main_hero, main_spring, enemy_hero, frame_no)
+            elif reward_name == "health_management":
+                reward_struct.cur_frame_value = self.calculate_health_management_reward(main_hero)
+            elif reward_name == "combat_timing":
+                reward_struct.cur_frame_value = self.calculate_combat_timing_reward(
+                    main_hero, enemy_hero)
+            elif reward_name == "tower_defense":
+                reward_struct.cur_frame_value = self.calculate_tower_defense_reward(
+                    main_hero, main_tower, enemy_hero)
+            elif reward_name == "skill_hit":
+                reward_struct.cur_frame_value = self.calculate_skill_hit_reward(
+                    frame_data.get("frame_action", {}))
+            elif reward_name == "positioning":
+                reward_struct.cur_frame_value = self.calculate_positioning_reward(
+                    main_hero, enemy_hero, main_tower)
+            # [修改点2] 新增小兵伤害奖励调用
+            elif reward_name == "minion_damage":
+                reward_struct.cur_frame_value = self.calculate_minion_damage_reward(
+                    frame_data, camp)
 
-    # Calculate the total amount of experience gained using agent level and current experience value
-    # 用智能体等级和当前经验值，计算获得经验值的总量
     def calculate_exp_sum(self, this_hero_info):
         exp_sum = 0.0
         for i in range(1, this_hero_info["level"]):
@@ -182,47 +546,51 @@ class GameRewardManager:
         exp_sum += this_hero_info["exp"]
         return exp_sum
 
-    # Calculate the forward reward based on the distance between the agent and both defensive towers
-    # 用智能体到双方防御塔的距离，计算前进奖励
-    def calculate_forward(self, main_hero, main_tower, enemy_tower):
+    def calculate_forward_improved(self, main_hero, main_tower, enemy_tower):
+        """改进的前进奖励计算"""
+        if not main_tower or not enemy_tower:
+            return 0.0
+        
         main_tower_pos = (main_tower["location"]["x"], main_tower["location"]["z"])
         enemy_tower_pos = (enemy_tower["location"]["x"], enemy_tower["location"]["z"])
-        hero_pos = (
-            main_hero["actor_state"]["location"]["x"],
-            main_hero["actor_state"]["location"]["z"],
-        )
+        hero_pos = (main_hero["actor_state"]["location"]["x"],
+                   main_hero["actor_state"]["location"]["z"])
+        
         forward_value = 0
         dist_hero2emy = math.dist(hero_pos, enemy_tower_pos)
         dist_main2emy = math.dist(main_tower_pos, enemy_tower_pos)
-        if main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"] > 0.99 and dist_hero2emy > dist_main2emy:
+        
+        hero_hp_ratio = main_hero["actor_state"]["hp"] / main_hero["actor_state"]["max_hp"]
+        
+        # 血量要求调整为40%
+        if hero_hp_ratio > 0.4 and dist_hero2emy < dist_main2emy:
             forward_value = (dist_main2emy - dist_hero2emy) / dist_main2emy
+            # 根据血量调整奖励
+            forward_value *= hero_hp_ratio
+        
         return forward_value
 
-    # Calculate the reward item information for both sides using frame data
-    # 用帧数据来计算两边的奖励子项信息
     def frame_data_process(self, frame_data):
         main_camp, enemy_camp = -1, -1
-
         for hero in frame_data["hero_states"]:
             if hero["player_id"] == self.main_hero_player_id:
                 main_camp = hero["actor_state"]["camp"]
                 self.main_hero_camp = main_camp
             else:
                 enemy_camp = hero["actor_state"]["camp"]
+        
         self.set_cur_calc_frame_vec(self.m_main_calc_frame_map, frame_data, main_camp)
         self.set_cur_calc_frame_vec(self.m_enemy_calc_frame_map, frame_data, enemy_camp)
 
-    # Use the values obtained in each frame to calculate the corresponding reward value
-    # 用每一帧得到的奖励子项信息来计算对应的奖励值
     def get_reward(self, frame_data, reward_dict):
         reward_dict.clear()
         reward_sum, weight_sum = 0.0, 0.0
+        
         for reward_name, reward_struct in self.m_cur_calc_frame_map.items():
+            # 原有奖励计算
             if reward_name == "hp_point":
-                if (
-                    self.m_main_calc_frame_map[reward_name].last_frame_value == 0.0
-                    and self.m_enemy_calc_frame_map[reward_name].last_frame_value == 0.0
-                ):
+                if (self.m_main_calc_frame_map[reward_name].last_frame_value == 0.0
+                    and self.m_enemy_calc_frame_map[reward_name].last_frame_value == 0.0):
                     reward_struct.cur_frame_value = 0
                     reward_struct.last_frame_value = 0
                 elif self.m_main_calc_frame_map[reward_name].last_frame_value == 0.0:
@@ -234,13 +602,12 @@ class GameRewardManager:
                 else:
                     reward_struct.cur_frame_value = (
                         self.m_main_calc_frame_map[reward_name].cur_frame_value
-                        - self.m_enemy_calc_frame_map[reward_name].cur_frame_value
-                    )
+                        - self.m_enemy_calc_frame_map[reward_name].cur_frame_value)
                     reward_struct.last_frame_value = (
                         self.m_main_calc_frame_map[reward_name].last_frame_value
-                        - self.m_enemy_calc_frame_map[reward_name].last_frame_value
-                    )
+                        - self.m_enemy_calc_frame_map[reward_name].last_frame_value)
                 reward_struct.value = reward_struct.cur_frame_value - reward_struct.last_frame_value
+            
             elif reward_name == "ep_rate":
                 reward_struct.cur_frame_value = self.m_main_calc_frame_map[reward_name].cur_frame_value
                 reward_struct.last_frame_value = self.m_main_calc_frame_map[reward_name].last_frame_value
@@ -248,6 +615,7 @@ class GameRewardManager:
                     reward_struct.value = reward_struct.cur_frame_value - reward_struct.last_frame_value
                 else:
                     reward_struct.value = 0
+            
             elif reward_name == "exp":
                 main_hero = None
                 for hero in frame_data["hero_states"]:
@@ -258,31 +626,30 @@ class GameRewardManager:
                 else:
                     reward_struct.cur_frame_value = (
                         self.m_main_calc_frame_map[reward_name].cur_frame_value
-                        - self.m_enemy_calc_frame_map[reward_name].cur_frame_value
-                    )
+                        - self.m_enemy_calc_frame_map[reward_name].cur_frame_value)
                     reward_struct.last_frame_value = (
                         self.m_main_calc_frame_map[reward_name].last_frame_value
-                        - self.m_enemy_calc_frame_map[reward_name].last_frame_value
-                    )
+                        - self.m_enemy_calc_frame_map[reward_name].last_frame_value)
                     reward_struct.value = reward_struct.cur_frame_value - reward_struct.last_frame_value
-            elif reward_name == "forward":
-                reward_struct.value = self.m_main_calc_frame_map[reward_name].cur_frame_value
-            elif reward_name == "last_hit":
+            
+            # [修改点2] 包含 minion_damage
+            elif reward_name in ["forward", "last_hit", "damage_exchange", "recall_decision",
+                                "health_management", "combat_timing", "tower_defense", 
+                                "skill_hit", "positioning", "minion_damage"]:
+                # 直接使用计算值
                 reward_struct.value = self.m_main_calc_frame_map[reward_name].cur_frame_value
             else:
-                # Calculate zero-sum reward
-                # 计算零和奖励
+                # 零和奖励
                 reward_struct.cur_frame_value = (
                     self.m_main_calc_frame_map[reward_name].cur_frame_value
-                    - self.m_enemy_calc_frame_map[reward_name].cur_frame_value
-                )
+                    - self.m_enemy_calc_frame_map[reward_name].cur_frame_value)
                 reward_struct.last_frame_value = (
                     self.m_main_calc_frame_map[reward_name].last_frame_value
-                    - self.m_enemy_calc_frame_map[reward_name].last_frame_value
-                )
+                    - self.m_enemy_calc_frame_map[reward_name].last_frame_value)
                 reward_struct.value = reward_struct.cur_frame_value - reward_struct.last_frame_value
 
             weight_sum += reward_struct.weight
             reward_sum += reward_struct.value * reward_struct.weight
             reward_dict[reward_name] = reward_struct.value
-        reward_dict["reward_sum"] = reward_sum
+        
+        reward_dict["reward_sum"] = reward_sum
\ No newline at end of file
diff --git a/agent_ppo/model/model.py b/agent_ppo/model/model.py
index e5b3310..b8c37a5 100644
--- a/agent_ppo/model/model.py
+++ b/agent_ppo/model/model.py
@@ -18,7 +18,7 @@ from math import ceil, floor
 from collections import OrderedDict
 from typing import Dict, List, Tuple
 
-from agent_ppo.conf.conf import DimConfig, Config
+from agent_diy.conf.conf import DimConfig, Config
 
 
 class Model(nn.Module):
@@ -38,6 +38,11 @@ class Model(nn.Module):
         self.is_reinforce_task_list = Config.IS_REINFORCE_TASK_LIST
         self.min_policy = Config.MIN_POLICY
         self.clip_param = Config.CLIP_PARAM
+        
+        # --- 已添加 ---
+        self.dual_clip_c = Config.DUAL_CLIP_PARAM_C
+        # --- 修改结束 ---
+        
         self.restore_list = []
         self.var_beta = self.m_var_beta
         self.learning_rate = self.m_learning_rate
@@ -507,13 +512,34 @@ class Model(nn.Module):
                 ratio = torch.exp(final_log_p)
                 clip_ratio = ratio.clamp(0.0, 3.0)
 
+                # --- Dual-Clip PPO 修改开始  ---
+                
+                # 1. 标准PPO裁剪 [cite: 216]
                 surr1 = clip_ratio * advantage
                 surr2 = ratio.clamp(1.0 - self.clip_param, 1.0 + self.clip_param) * advantage
+                standard_clip_surr = torch.minimum(surr1, surr2)
+
+                # 2. Dual-Clip PPO 下界 
+                #    对应论文中的 c * A_t，c 在 __init__ 中从 Config 加载 [cite: 236]
+                dual_clip_surr = self.dual_clip_c * advantage 
+                
+                # 3. 仅在 advantage < 0 时，应用 Dual-Clip [cite: 234]
+                #    当 A_t < 0 时，最终surrogate为 max(standard_clip, dual_clip_bound) 
+                #    当 A_t >= 0 时，最终surrogate为 standard_clip
+                final_surr = torch.where(
+                    advantage < 0,
+                    torch.maximum(standard_clip_surr, dual_clip_surr),
+                    standard_clip_surr
+                )
+                
+                # 4. 计算最终损失
                 temp_policy_loss = -torch.sum(
-                    torch.minimum(surr1, surr2) * (weight_list[task_index].float()) * 1
+                    final_surr * (weight_list[task_index].float()) * 1
                 ) / torch.maximum(torch.sum((weight_list[task_index].float()) * 1), torch.tensor(1.0))
 
                 self.policy_cost = self.policy_cost + temp_policy_loss
+                
+                # --- Dual-Clip PPO 修改结束 ---
 
         # cross entropy loss
         # 交叉熵损失
@@ -623,4 +649,4 @@ class MLP(nn.Module):
                 self.fc_layers.add_module("{0}_non_linear{1}".format(name, i + 1), non_linearity())
 
     def forward(self, data):
-        return self.fc_layers(data)
+        return self.fc_layers(data)
\ No newline at end of file
diff --git a/agent_ppo/workflow/train_workflow.py b/agent_ppo/workflow/train_workflow.py
index af16ba7..268ac39 100644
--- a/agent_ppo/workflow/train_workflow.py
+++ b/agent_ppo/workflow/train_workflow.py
@@ -11,14 +11,14 @@ Author: Tencent AI Arena Authors
 import os
 import time
 import random
-from agent_ppo.feature.definition import (
+from agent_diy.feature.definition import (
     sample_process,
     build_frame,
     FrameCollector,
     NONE_ACTION,
 )
 from kaiwu_agent.utils.common_func import attached
-from agent_ppo.conf.conf import GameConfig
+from agent_diy.conf.conf import GameConfig
 from tools.model_pool_utils import get_valid_model_pool
 from kaiwudrl.common.checkpoint.model_file_sync import ModelFileSync
 from tools.train_env_conf_validate import read_usr_conf, check_usr_conf
@@ -39,16 +39,16 @@ def workflow(envs, agents, logger=None, monitor=None):
 
     # Read and validate configuration file
     # 配置文件读取和校验
-    usr_conf = read_usr_conf("agent_ppo/conf/train_env_conf.toml", logger)
+    usr_conf = read_usr_conf("agent_diy/conf/train_env_conf.toml", logger)
     if usr_conf is None:
-        logger.error(f"usr_conf is None, please check agent_ppo/conf/train_env_conf.toml")
+        logger.error(f"usr_conf is None, please check agent_diy/conf/train_env_conf.toml")
         return
     # check_usr_conf is a tool to check whether the environment configuration is correct
     # It is recommended to perform a check before calling reset.env
     # check_usr_conf会检查环境配置是否正确，建议调用reset.env前先检查一下
     valid = check_usr_conf(usr_conf, logger)
     if not valid:
-        logger.error(f"check_usr_conf return False, please check agent_ppo/conf/train_env_conf.toml")
+        logger.error(f"check_usr_conf return False, please check agent_diy/conf/train_env_conf.toml")
         return
 
     while True:
@@ -139,6 +139,9 @@ def run_episodes(envs, agents, logger, monitor, model_file_sync_wrapper, usr_con
             is_eval = (episode_cnt + random_eval_start) % eval_interval == 0
         if is_eval:
             opponent_agent = usr_conf["env_conf"]["episode"]["eval_opponent_type"]
+            #eval_opponent_list = ["142806","143126"]
+            #opponent_agent = random.choice(eval_opponent_list)
+            #logger.info(f"This is an eval game, random opponent: {opponent_agent}")
         usr_conf["env_conf"]["episode"]["opponent_agent"] = opponent_agent
 
         # Start a new environment
diff --git a/conf/configure_app.toml b/conf/configure_app.toml
index f4c1b57..5fc7784 100644
--- a/conf/configure_app.toml
+++ b/conf/configure_app.toml
@@ -23,7 +23,7 @@ run_mode = "train"
 
 # Algorithm used
 # 采用的算法
-algo = "ppo"
+algo = "diy"
 
 # Model file synchronization interval between learner/actor (recommended to be even multiples)
 # learner/actor之间同步model文件的时间间隔, 建议是偶数倍
diff --git a/conf/kaiwudrl/configure.toml b/conf/kaiwudrl/configure.toml
index 3886d7d..6254d82 100644
--- a/conf/kaiwudrl/configure.toml
+++ b/conf/kaiwudrl/configure.toml
@@ -271,9 +271,9 @@ framework_integration_patterns = "normal"
 # 采用的wrapper形式, 包括remote, local, none
 wrapper_type = "remote"
 # 用户保存模型最大次数, 设置为小于等于0代表不限制
-user_save_mode_max_count = 0
+user_save_mode_max_count = 400
 # 用户保存模型的频率, 设置为小于等于0代表不限制
-user_save_model_max_frequency_per_min = 0
+user_save_model_max_frequency_per_min = 2
 # 在模型文件保存时, 需要保存的文件目录, 多个目录请按照逗号分割, 并且是以项目根目录开始看的
 copy_dir = "conf,"
 # 标准化模式下拷贝到目标文件夹用于COS上传的目录, 和开悟平台对齐
diff --git a/kaiwu.json b/kaiwu.json
index 4e87daf..e7ad118 100644
--- a/kaiwu.json
+++ b/kaiwu.json
@@ -1 +1 @@
-{"model_pool": []}
+{"model_pool": [144259]}
diff --git a/train_test.py b/train_test.py
index 35054c9..c573cd1 100644
--- a/train_test.py
+++ b/train_test.py
@@ -29,7 +29,7 @@ from typing import List
 # Simply modify the value of the algorithm_name variable.
 # 运行train_test前必须修改这里的算法名字, 必须是ppo、diy里的一个, 修改algorithm_name的值即可
 algorithm_name_list = ["ppo", "diy"]
-algorithm_name = "ppo"
+algorithm_name = "diy"
 
 
 # train
